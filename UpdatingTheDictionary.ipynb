{"cells":[{"cell_type":"markdown","metadata":{"id":"duq4gOzIpp7F"},"source":["# Updating the Dictionary\n","The following project attempts to create a Boolean Retrieval system able to answer binary and phrase queries. Users should also be able to add and delete documents from the system.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"MnJXnmpdpp7J","executionInfo":{"status":"ok","timestamp":1705788251742,"user_tz":-60,"elapsed":304,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"outputs":[],"source":["from functools import total_ordering, reduce\n","import csv  # Import the csv module for CSV file parsing\n","import re  # Import the re module for regular expression operations\n"]},{"cell_type":"code","source":["# Needed to execute the notebook on google colab\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFm6seGN-oof","executionInfo":{"status":"ok","timestamp":1705788255249,"user_tz":-60,"elapsed":1868,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"98860c11-5e3a-4b4c-a1f8-f3a0b2f3972d"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"ty7C6ESQpp7L"},"source":["## Construction of the IRsystem\n","\n","In order to tackle the problem different classes have to be implemented. Specifically:\n","\n","\n","*   **Posting** class: it represents a posting in an index\n","*   **PostingList** class: it represents a list of postings in an index\n","*   **Terms** class: it represents a term in an index, which is associated with its posting list\n","*   **PositionalIndex** class: it represents the index, which is a positional index given that the positions where each term appears in each document are saved in order to answer phrase queries\n","*   **IRsystem** class: it represents the structure of the whole IR system. It is used to initialize the system, add or delete documents and perform queries.\n","\n","Let us now inspect these classes\n"]},{"cell_type":"markdown","source":["### Postings"],"metadata":{"id":"_CCRQ3vAMkIH"}},{"cell_type":"code","execution_count":73,"metadata":{"id":"qBhikJmwpp7L","executionInfo":{"status":"ok","timestamp":1705788260674,"user_tz":-60,"elapsed":329,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"outputs":[],"source":["@total_ordering  # This decorator will add all rich comparison methods based on the definitions of __eq__ and __gt__.\n","class Posting:    # The class represents a 'Posting' in an index\n","\n","    def __init__(self, docID, positions):\n","        # The initializer method for the class, which takes a document ID as an argument.\n","        self._docID = docID  # The document ID is stored in a protected member variable.\n","        self._positions= positions # The positions in the document where the term appears\n","\n","    def get_from_corpus(self, corpus):\n","        # A method to retrieve a document's contents from a corpus using the stored document ID.\n","        return corpus[self._docID]  # Returns the document associated with the document ID from the corpus.\n","\n","    def __eq__(self, other):\n","        # Special method to check equality with another Posting, based on document ID.\n","        return self._docID == other._docID  # Returns True if the document IDs are equal, otherwise False.\n","\n","    def __gt__(self, other):\n","        # Special method to check if this Posting is greater than another Posting, based on document ID.\n","        return self._docID > other._docID  # Returns True if this document ID is greater than the other's.\n","\n","    def __repr__(self):\n","        # Special method to provide the official string representation of the Posting.\n","        return f\"DocID: {self._docID}, Positions: {', '.join(map(str, self._positions))}\"\n"]},{"cell_type":"markdown","metadata":{"id":"lLw9bNHLpp7M"},"source":["### Posting Lists"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"u81tZ4y3pp7M","executionInfo":{"status":"ok","timestamp":1705788261327,"user_tz":-60,"elapsed":5,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"outputs":[],"source":["class PostingList:\n","    # This class represents a list of postings\n","\n","    def __init__(self):\n","        # The initializer method for the class. It initializes an empty list of postings.\n","        self._postings = []  # Protected member variable that holds the list of postings.\n","\n","    @classmethod\n","    def from_docID(cls, docID, position):\n","        # A class method to create a PostingList instance with a single Posting from a document ID and the position where the term was found.\n","        plist = cls()  # Creates a new instance of the class.\n","        plist._postings = [(Posting(docID, [position]))]  # Initializes the postings list with a single Posting and the position where the term was first found.\n","        return plist  # Returns the newly created PostingList instance.\n","\n","    @classmethod\n","    def from_posting_list(cls, postingList):\n","        # A class method to create a PostingList instance from an existing list of Postings.\n","        plist = cls()  # Creates a new instance of the class.\n","        plist._postings = postingList  # Sets the postings list to the provided list.\n","        return plist  # Returns the newly created PostingList instance.\n","\n","    def merge(self, other):\n","        # A method to merge another PostingList into this one, avoiding duplicates.\n","        i = 0  # Start index for the other PostingList.\n","        last = self._postings[-1] if self._postings else None  # The last Posting in the current list.\n","        while i < len(other._postings):\n","            current_posting = other._postings[i]\n","            # Check if there's a matching posting with the same document ID.\n","            if last is None or last._docID < current_posting._docID:\n","                # If no match is found, append the posting to the current list.\n","                self._postings.append(current_posting)\n","                last = current_posting\n","            elif last._docID == current_posting._docID:\n","                # If a match is found, add new positions to the existing posting.\n","                existing_posting = self._postings[-1]\n","                existing_posting._positions.extend(current_posting._positions)\n","            i += 1  # Move to the next posting in the other PostingList.\n","\n","    def intersection(self, other):\n","        # A method to compute the intersection of this PostingList with another.\n","        intersection = []  # Start with an empty list for the intersection.\n","        i = 0  # Index for this PostingList.\n","        j = 0  # Index for the other PostingList.\n","        # Loop until one of the lists is exhausted.\n","        while (i < len(self._postings) and j < len(other._postings)):\n","            # If both postings are equal, add to the intersection.\n","            if (self._postings[i] == other._postings[j]):\n","                intersection.append(self._postings[i])\n","                i += 1\n","                j += 1\n","            # If the current posting is less, increment this list's index.\n","            elif (self._postings[i] < other._postings[j]):\n","                i += 1\n","            # If the other posting is less, increment the other list's index.\n","            else:\n","                j += 1\n","        return PostingList.from_posting_list(intersection)  # Return a new PostingList of the intersection.\n","\n","    def union(self, other):\n","        # A method to compute the union of this PostingList with another.\n","        union = []  # Start with an empty list for the union.\n","        i = 0  # Index for this PostingList.\n","        j = 0  # Index for the other PostingList.\n","        # Loop until one of the lists is exhausted.\n","        while (i < len(self._postings) and j < len(other._postings)):\n","            # If both postings are equal, add to the union and increment both indexes.\n","            if (self._postings[i] == other._postings[j]):\n","                union.append(self._postings[i])\n","                i += 1\n","                j += 1\n","            # If the current posting is less, add it to the union and increment this list's index.\n","            elif (self._postings[i] < other._postings[j]):\n","                union.append(self._postings[i])\n","                i += 1\n","            # Otherwise, add the other posting to the union and increment the other list's index.\n","            else:\n","                union.append(other._postings[j])\n","                j += 1\n","        # Add any remaining postings from both lists to the union.\n","        for k in range(i, len(self._postings)):\n","            union.append(self._postings[k])\n","        for k in range(j, len(other._postings)):\n","            union.append(other._postings[k])\n","        return PostingList.from_posting_list(union)  # Return a new PostingList of the union.\n","\n","    def difference(self, other):\n","        # A method to compute the difference of this PostingList with another.\n","        diff = []  # Start with an empty list for the difference.\n","        i = 0  # Index for this PostingList.\n","        j = 0  # Index for the other PostingList.\n","        # Loop until one of the lists is exhausted.\n","        while (i < len(self._postings) and j < len(other._postings)):\n","            # If both postings are equal, skip this posting and increment both indexes.\n","            if self._postings[i] == other._postings[j]:\n","                i += 1\n","                j += 1\n","            # If the current posting is less, add it to the difference and increment this list's index.\n","            elif self._postings[i] < other._postings[j]:\n","                diff.append(self._postings[i])\n","                i += 1\n","            # If the other posting is less, skip it and increment the other list's index.\n","            else:\n","                j += 1\n","        # Add any remaining postings from this list to the difference.\n","        for k in range(i, len(self._postings)):\n","            diff.append(self._postings[k])\n","        return PostingList.from_posting_list(diff)  # Return a new PostingList of the difference.\n","\n","    def get_from_corpus(self, corpus):\n","        # A method to retrieve the contents of each Posting from a corpus.\n","        return list(map(lambda x: x.get_from_corpus(corpus), self._postings))  # Use map to apply the retrieval to each Posting.\n","\n","    def __repr__(self):\n","        # Special method to provide the official string representation of the PostingList.\n","        return \"/\".join(map(str, self._postings))"]},{"cell_type":"markdown","metadata":{"id":"5wU2rlGXpp7N"},"source":["### Terms"]},{"cell_type":"code","execution_count":75,"metadata":{"jupyter":{"source_hidden":true},"id":"1ptMmcNgoFes","executionInfo":{"status":"ok","timestamp":1705788261917,"user_tz":-60,"elapsed":2,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"outputs":[],"source":["# Define a custom exception class for handling errors specific to merge operations.\n","class ImpossibleMergeError(Exception):\n","    pass\n","\n","# The total_ordering decorator will automatically provide the other comparison methods based on __eq__ and __gt__.\n","@total_ordering\n","class Term:\n","    # A class that represents a term in a document, along with its posting list.\n","\n","    def __init__(self, term, docID, position):\n","        # The initializer method for the class, taking a term and a document ID as arguments.\n","        self.term = term  # Public attribute to store the term.\n","        # Initialize posting_list for the term with a PostingList created from the given document ID.\n","        self.posting_list = PostingList.from_docID(docID,position)\n","\n","    def merge(self, other):\n","        # A method to merge another Term's posting list into this one if they have the same term.\n","        if (self.term == other.term):\n","            # If terms match, merge the posting lists.\n","            self.posting_list.merge(other.posting_list)\n","        else:\n","            # If terms don't match, it's not possible to merge, so raise an exception.\n","            raise ImpossibleMergeError\n","\n","    def __eq__(self, other):\n","        # Special method to check equality with another Term based on the term string.\n","        return self.term == other.term  # Comparison is done lexicographically.\n","\n","    def __gt__(self, other):\n","        # Special method to determine if this Term is greater than another, based on the term string.\n","        return self.term > other.term  # Comparison is done lexicographically.\n","\n","    def __repr__(self):\n","        # Special method to provide the official string representation of the Term.\n","        return f\"{self.term}: {self.posting_list.__repr__()}\"\n"]},{"cell_type":"markdown","metadata":{"id":"T-_DNEW8pp7P"},"source":["### Positional Index"]},{"cell_type":"code","execution_count":76,"metadata":{"jupyter":{"source_hidden":true},"id":"GiyxYm-GoFet","executionInfo":{"status":"ok","timestamp":1705788262307,"user_tz":-60,"elapsed":3,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"outputs":[],"source":["# Function to normalize text by removing punctuation, converting to lowercase.\n","def normalize(text):\n","    # Removes punctuation from the text using a regular expression.\n","    no_punctuation = re.sub(r'[^\\w\\s^-]', '', text)\n","    # Converts the text to lowercase.\n","    downcase = no_punctuation.lower()\n","    # Returns the normalized text.\n","    return downcase\n","\n","# Function to tokenize the description of a movie into individual words.\n","def tokenize(movie):\n","    # Normalize the movie description.\n","    text = normalize(movie.description)\n","    # Split the text into a list of tokens (words) and return it.\n","    return list(text.split())\n","\n","# Define a class that represents an inverted index.\n","class PositionalIndex:\n","\n","    def __init__(self):\n","        # Initialize the inverted index with an empty dictionary.\n","        self._dictionary = []\n","\n","    # Class method to create an inverted index from a corpus of documents.\n","    @classmethod\n","    def from_corpus(cls, corpus):\n","        # Create an intermediate dictionary to store terms and their postings.\n","        intermediate_dict = {}\n","        # Iterate over the documents in the corpus.\n","        for document in corpus:\n","            # Tokenize the document into individual words.\n","            tokens = tokenize(document)\n","            for index, token in enumerate(tokens):\n","                term = Term(token, document.docID, index)\n","                try:\n","                    # Try to merge the term with existing one in the intermediate dictionary.\n","                    intermediate_dict[token].merge(term)\n","                except KeyError:\n","                    # If the term is not already in the dictionary, add it.\n","                    intermediate_dict[token] = term\n","            # Print progress for every 1000 documents processed.\n","            if (document.docID % 1000 == 0):\n","                print(\"ID: \" + str(document.docID))\n","        # Create a new PositionalIndex instance.\n","        idx = cls()\n","        # Sort the terms in the intermediate dictionary and store them in the index's dictionary.\n","        idx._dictionary = sorted(intermediate_dict.values(), key=lambda term: term.term)\n","        # Return the newly created inverted index.\n","        return idx\n","\n","    # Method to merge indexes\n","    def merge(self, other):\n","        merged_dict = {}\n","        l1 = len(self._dictionary)\n","        l2 = len(other._dictionary)\n","        i=0\n","        j=0\n","        while i<l1 and j<l2:\n","            if self._dictionary[i].term == other._dictionary[j].term:\n","                # Terms match, merge their posting lists.\n","                self._dictionary[i].posting_list.merge(other._dictionary[j].posting_list)\n","                merged_dict[self._dictionary[i].term]=self._dictionary[i]\n","                i+=1\n","                j+=1\n","            elif self._dictionary[i].term < other._dictionary[j].term:\n","                # Term in dict1 is smaller, add it to the merged_dict.\n","                merged_dict[self._dictionary[i].term]=self._dictionary[i]\n","                i+=1\n","            else:\n","                # Term in dict2 is smaller, add it to the merged_dict.\n","                merged_dict[other._dictionary[j].term]=other._dictionary[j]\n","                j+=1\n","        # Add any remaining terms from dict1 or dict2.\n","        while i<l1:\n","            merged_dict[self._dictionary[i].term]=self._dictionary[i]\n","            i+=1\n","        while j<l2:\n","            merged_dict[other._dictionary[j].term]=other._dictionary[j]\n","            j+=1\n","        # Create a new InvertedIndex instance.\n","        idx = PositionalIndex()\n","        # Sort the terms in the intermediate dictionary and store them in the index's dictionary.\n","        idx._dictionary = sorted(merged_dict.values(), key=lambda term: term.term)\n","        # Return the newly created inverted index.\n","        return idx\n","\n","    # Special method to retrieve the posting list for a given term.\n","    def __getitem__(self, key):\n","        # Iterate over the terms in the dictionary.\n","        for term in self._dictionary:\n","            # If the term matches the key, return its posting list.\n","            if term.term == key:\n","                return term\n","        # If the term is not found, raise a KeyError.\n","        raise KeyError\n","\n","    # Special method to provide a string representation of the inverted index.\n","    def __repr__(self):\n","        # Returns a string indicating the number of terms in the dictionary.\n","        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms\"\n"]},{"cell_type":"markdown","metadata":{"id":"9V_-ECH8pp7Q"},"source":["### Reading the Corpus\n","In this section, essential functions for processing the original text and converting it into documents are provided.\n","\n","The **MovieDescription** class is used to represent movie objects, each identified by a unique ID, title, and description.\n","\n","The **read_movie_description** function is needed to transform a corpus of documents into a list of **MovieDescription** objects. This function accepts three input variables:\n","\n","*   **percentage**: Determines the proportion of the original corpus to be read, influencing the number of documents in the final corpus returned by the function.\n","*   **start_percentage**: Specifies the initial point for reading documents, allowing users to start from the beginning of the provided text or any desired position.\n","*   **rename**: A boolean variable used to reset the document IDs. Typically, the function assigns IDs based on the order of reading. However, setting rename to True enables users to create a new system with a subset of documents from the main corpus, with their document IDs reset to start from 0.\n","\n","\n","\n","To successfully execute this notebook, you may need to make adjustments to the filenames, ensuring that they accurately specify the correct file paths."]},{"cell_type":"code","execution_count":77,"metadata":{"jupyter":{"source_hidden":true},"id":"7WVom0aNoFet","executionInfo":{"status":"ok","timestamp":1705788262308,"user_tz":-60,"elapsed":4,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"outputs":[],"source":["# Define a class to hold the title and description of a movie.\n","class MovieDescription:\n","\n","    def __init__(self, title, description, docID):\n","        # Constructor for the class that initializes the title and description attributes.\n","        self.docID = docID\n","        self.title = title\n","        self.description = description\n","\n","    def __repr__(self):\n","        # Special method to provide the string representation of the MovieDescription object.\n","        # It returns the movie's title when the object is printed or shown in the interpreter.\n","        return self.title\n","\n","\n","# Define a function to read movie descriptions and titles from files.\n","def read_movie_descriptions(percentage, start_percentage, rename=False):\n","    # Names of the files containing plot summaries and metadata respectively.\n","    filename = '/content/gdrive/My Drive/Information retrieval/plot_summaries.txt' # Change the path to your specific path to the file\n","    movie_names_file = '/content/gdrive/My Drive/Information retrieval/movie.metadata.tsv' # Change the path to your specific path to the file\n","    # Open the movie metadata file and read it line by line up to the desired percentage.\n","    with open(movie_names_file, 'r', encoding=\"utf8\") as csv_file:\n","        # Create a csv.reader object to read the file with tab as the delimiter.\n","        movie_names = csv.reader(csv_file, delimiter='\\t')\n","        # Initialize a dictionary to hold movie IDs and their corresponding titles.\n","        names_table = {}\n","        for name in movie_names:\n","            # Populate the dictionary with movie ID as key and title as value.\n","            names_table[name[0]] = name[2]\n","    # Open the file containing plot summaries and read it line by line.\n","    with open(filename, 'r', encoding=\"utf8\") as csv_file:\n","        # Create a csv.reader object to read the file with tab as the delimiter.\n","        descriptions = csv.reader(csv_file, delimiter='\\t')\n","        # Initialize a list to hold the corpus of movie descriptions.\n","        corpus = []\n","        docID= 0\n","        for desc in descriptions:\n","            try:\n","                # Create a MovieDescription object using the title from names_table and the description from the file.\n","                movie = MovieDescription(names_table[desc[0]], desc[1], docID)\n","                # Add the MovieDescription object to the corpus.\n","                corpus.append(movie)\n","                # Update the counter of read documents\n","                docID+=1\n","            except KeyError:\n","                # If the movie ID is not found in names_table, ignore this description.\n","                pass\n","        total_lines=len(corpus)\n","        corpus=corpus[int(start_percentage*total_lines):int((start_percentage+percentage)*total_lines) ]\n","        if(rename):\n","            for index, doc in enumerate(corpus):\n","                doc.docID= index\n","        # Return the populated list of MovieDescription objects.\n","        return corpus\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XWuqzHRTpp7R"},"source":["### Putting it all together"]},{"cell_type":"code","execution_count":112,"metadata":{"jupyter":{"source_hidden":true},"id":"2qQVjxvRoFeu","executionInfo":{"status":"ok","timestamp":1705790428316,"user_tz":-60,"elapsed":302,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"outputs":[],"source":["import copy\n","# Define a class for an Information Retrieval (IR) system.\n","class IRsystem:\n","\n","    def __init__(self, corpus, index, invalidation_vector):\n","        # Initialize the IR system with a corpus (collection of documents) and the inverted index.\n","        self._corpus = copy.deepcopy(corpus)  # The corpus of documents.\n","        self._index = index  # The main inverted index for the corpus.\n","        self._auxiliary_index = PositionalIndex() # The auxiliary index\n","        self._merged = True # Boolean variable that says if the indexes have been merged\n","        self._auxiliary_corpus = [] # The corpus of added documents\n","        self._invalidation_vector = invalidation_vector  # The invalidation vector.\n","\n","    @classmethod\n","    def from_corpus(cls, corpus):\n","        # Class method to create an IR system instance from a given corpus.\n","        # It creates an inverted index from the corpus first.\n","        index = PositionalIndex.from_corpus(corpus)\n","        invalidation_vector= [True]*len(corpus)\n","        # Returns an instance of the IR system with the given corpus and created index.\n","        return cls(corpus, index, invalidation_vector)\n","\n","    @classmethod\n","    def from_index_file(cls, filename, corpus):\n","        index = PositionalIndex()\n","        # Create an intermediate dictionary to store terms and their postings.\n","        intermediate_dict = {}\n","        #read the index from filename\n","        with open(filename, 'r', encoding='utf-8') as txtfile:\n","            # Read lines from the file\n","            lines = txtfile.readlines()\n","            # Get the first line\n","            first_row = lines[0].strip()\n","            # Convert characters back to booleans\n","            invalidation_vector = [bool(int(char)) for char in first_row]\n","            # Remove the first line\n","            lines = lines[1:]\n","            # Iterate over the remaining lines (starting from the second line)\n","            for line in lines:\n","                # Split the line into term and posting list\n","                term, posting_list_str = line.split(': ', 1)\n","                # Create Term object\n","                term = Term(term, 0, 0)\n","                # Split the posting list into individual postings\n","                posting_entries = posting_list_str.split('/')\n","                # Process each posting entry\n","                postings = []\n","                for entry in posting_entries:\n","                    # Extract DocID and Positions\n","                    docID_str, positions_str = entry.split(', Positions: ')\n","                    docID = int(docID_str.split(': ')[1])\n","                    positions = list(map(int, positions_str.split(',')))\n","                    # Create Posting object\n","                    posting = Posting(docID, positions)\n","                    # Add the Posting to the postings list\n","                    postings.append(posting)\n","                # Create PostingList object\n","                postings = PostingList.from_posting_list(postings)\n","                # Set the term's posting list\n","                term.posting_list = postings\n","                # Add the term to the index\n","                intermediate_dict[term.term] = term\n","        # Set the dictionary\n","        index._dictionary = sorted(intermediate_dict.values(), key=lambda term: term.term)\n","        # Returns an instance of the IR system with the given corpus and the loaded index.\n","        return cls(corpus, index, invalidation_vector)\n","\n","    # Method to add documents to the auxiliary index\n","    def add_documents(self, corpus):\n","        # Loop through the list of documents\n","        self._auxiliary_corpus= []\n","        for movie in corpus:\n","            # Check if the added document is new or if it is a document that was previously deleted\n","            if(movie.docID)>=len(self._corpus):\n","                self._corpus.append(MovieDescription(movie.title, movie.description, movie.docID))\n","                self._auxiliary_corpus.append(MovieDescription(movie.title, movie.description, movie.docID))\n","                self._invalidation_vector.append(True)\n","            else:\n","                self._invalidation_vector[movie.docID]= True\n","                self._corpus[movie.docID] = MovieDescription(movie.title, movie.description, movie.docID)\n","        self._auxiliary_index = self._auxiliary_index.from_corpus(self._auxiliary_corpus)\n","        if(len(self._auxiliary_corpus)==0):\n","            self._merged= True\n","        else:\n","            self._merged = False\n","\n","    # Method to delete documents\n","    def delete_documents(self, corpus):\n","        for movie in corpus:\n","            self._invalidation_vector[movie.docID]=False\n","            self._corpus[movie.docID] = None\n","\n","    # Method to merge main and auxiliary indexes\n","    def merge_indexes(self):\n","        self._index = self._index.merge(self._auxiliary_index)\n","        self._auxiliary_index = PositionalIndex()\n","        self._auxiliary_corpus = []\n","        self._merged = True\n","\n","    # Method to retrieve valid corpus\n","    def get_valid_corpus(self):\n","        valid_docs=[]\n","        for movie in self.corpus:\n","            if(self._corpus[movie.docID] != None):\n","                valid_docs.append(movie)\n","        return valid_docs\n","\n","    # Method to save the index\n","    def save_to_txt(self, filename):\n","        with open(filename, 'w', encoding='utf-8') as txtfile:\n","            # Convert boolean values to integers and join into a string\n","            invalidation_vector = ''.join(str(int(value)) for value in self._invalidation_vector)\n","            # Write the invalidation_vector in the first line\n","            txtfile.write(invalidation_vector+'\\n')\n","            # Write data\n","            for term in self._index._dictionary:\n","                txtfile.write(term.__repr__() + '\\n')\n","\n","    # Method to combine postings lists according to the binary operator\n","    def combine_postings(self,stack):\n","        if not stack:\n","            return None\n","        result = None\n","        # Process the inner stacks recursively\n","        for i in range(len(stack)):\n","            if isinstance(stack[i], list):\n","                stack[i] = self.combine_postings(stack[i])\n","        # Combine the parsed expressions and operators\n","        result = stack[0]\n","        for i in range(1, len(stack), 2):\n","            if i + 1 < len(stack):\n","                operator, operand = stack[i], stack[i + 1]\n","                if operator == 'AND':\n","                    result = result.intersection(operand)\n","                elif operator == 'OR':\n","                    result = result.union(operand)\n","                elif operator == 'NOT':\n","                    result = result.difference(operand)\n","                else:\n","                    raise ValueError(\"Missing operator.\")\n","            else:\n","                break\n","        return result\n","\n","\n","    # Method to organize the words/operations\n","    def parse_tokens(self, tokens, index):\n","        stack = []\n","        current_operator = None\n","        # Loop through the words\n","        i=0\n","        while(i<len(tokens)):\n","            if tokens[i].upper() in ['AND', 'OR', 'NOT']:\n","                current_operator = tokens[i].upper()\n","                stack.append(current_operator)\n","                i+=1\n","            elif tokens[i] == '(':\n","                stack2= []\n","                # Recursive call for expressions inside parentheses\n","                sub_expression = self.parse_tokens(tokens[i+1:], index)\n","                for token1 in sub_expression:\n","                    stack2.append(token1)\n","                stack.append(stack2)\n","                i=i+len(stack2)+2 # Skip to the expressions outside the parentheses\n","            elif tokens[i] == ')':\n","                return stack  # return the inner stack when closing parenthesis is encountered\n","            else:\n","                # Single term\n","                term_postings = index[tokens[i]].posting_list\n","                stack.append(term_postings)\n","                current_operator = None\n","                i+=1\n","        return stack\n","\n","\n","     # Method to answer queries with binary operations\n","    def answer_binary_query(self, words):\n","        # Use the parse_tokens function to get the posting lists of the terms from the main index and the operations\n","        tokens=self.parse_tokens(words, self._index)\n","        # Combine posting lists and operations\n","        postings = self.combine_postings(tokens)\n","        if not self._merged:\n","            # Use the parse_tokens function to get the posting lists of the terms from the auxiliary index and the operations\n","            tokens_aux = self.parse_tokens(words, self._auxiliary_index)\n","            # Combine posting lists and operations\n","            postings_aux = self.combine_postings(tokens_aux)\n","            # Combine the posting lists from both indexes.\n","            postings= postings.union(postings_aux)\n","        # Filter the postings list to exclude deleted documents\n","        valid_documents = [doc for doc in postings._postings if self._invalidation_vector[doc._docID]]\n","        # Return the list of documents from the corpus that match all query words.\n","        valid_documents = PostingList.from_posting_list(valid_documents)\n","        return valid_documents.get_from_corpus(self._corpus)\n","\n","\n","    # Method to retrieve the postings list of phrase queries, given the words and index to use\n","    def retrieve_phrase_posting_lists(self, words, index, proximity=1):\n","        # Normalize the words in the query to match the normalized index terms.\n","        norm_words = list(map(normalize, words))\n","        lw=len(norm_words)\n","        # Retrieve the posting lists for each normalized word from the index.\n","        postings = list(map(lambda w: index[w].posting_list, norm_words))\n","        # Initialize the intersection list to the posting list of the first term\n","        intersection = PostingList.from_posting_list(copy.deepcopy(postings[0]._postings))\n","        # Loop through the words\n","        for k in range(0,lw-1):\n","            i=len(intersection._postings) - 1\n","            j=len(postings[k+1]._postings) - 1\n","            # Loop through the postings lists in reverse\n","            while (i >= 0 and j >= 0):\n","                # If both postings docIDs are equal, check the positions.\n","                if (intersection._postings[i]._docID == postings[k+1]._postings[j]._docID):\n","                    #Loop through the positions in reverse\n","                    ii=len(intersection._postings[i]._positions) - 1\n","                    jj=len(postings[k+1]._postings[j]._positions) - 1\n","                    while (ii >= 0 and jj >= 0):\n","                        if(intersection._postings[i]._positions[ii]+ proximity + k == postings[k+1]._postings[j]._positions[jj]):\n","                            # You can leave the posting with this docID and position in the intersection, so simply skip to the next iteration\n","                            ii-=1\n","                            jj-=1\n","                        elif (intersection._postings[i]._positions[ii] > postings[k+1]._postings[j]._positions[jj]):\n","                            # Remove the position from the intersection as there are no matching subsequent words\n","                            intersection._postings[i]._positions.pop(ii)\n","                            ii-=1\n","                        else:\n","                            jj-=1\n","                    while (ii >= 0):\n","                        # Remove the positions from the intersection as there are no matching positions for the subsequent words\n","                        intersection._postings[i]._positions.pop(ii)\n","                        ii -= 1\n","                    # Check if the document no longer has any valid positions and, if so, remove it from the list\n","                    if(len(intersection._postings[i]._positions)==0):\n","                        intersection._postings.pop(i)\n","                    i-=1\n","                    j-=1\n","                elif(intersection._postings[i]._docID > postings[k+1]._postings[j]._docID):\n","                    # Remove the document from the intersection as there are no matching documents for the subsequent words\n","                    intersection._postings.pop(i)\n","                    i -= 1\n","                else:\n","                    j -= 1\n","            while (i >= 0):\n","                # Remove the document from the intersection as there are no matching documents for the subsequent words\n","                intersection._postings.pop(i)\n","                i -= 1\n","        return intersection\n","\n","    # Method to answer phrase queries\n","    def answer_phrase_query(self, words, proximity=1):\n","        # Use the retrieve_phrase_posting_lists function for the main index.\n","        postings = self.retrieve_phrase_posting_lists(words, self._index)\n","        # Check if the indexes have been merged.\n","        if not self._merged:\n","            # Use the retrieve_phrase_posting_lists function for the auxiliary index.\n","            postings_aux = self.retrieve_phrase_posting_lists(words, self._auxiliary_index)\n","            # Combine the posting lists from both indexes.\n","            postings = postings.union(postings_aux)\n","        # Filter the postings list to exclude deleted documents\n","        valid_documents = [doc for doc in postings._postings if self._invalidation_vector[doc._docID]]\n","        # Return the list of documents from the corpus that match all query words.\n","        valid_documents = PostingList.from_posting_list(valid_documents)\n","        return valid_documents.get_from_corpus(self._corpus)\n","\n","\n","\n","\n","\n","# Function to execute a query with binary operations against an IR system.\n","def binary_query(ir, text):\n","    # Split the text query into individual words.\n","    words = text.split()\n","    # Get the answer to the query using the IR system's parse_tokens method.\n","    answer = ir.answer_binary_query(words)\n","    # Print out each movie that matches the query.\n","    for movie in answer:\n","        print(movie)\n","\n","\n","# Function to execute a text query against an IR system.\n","def phrase_query(ir, text):\n","    # Split the text query into individual words.\n","    words = text.split()\n","    # Get the answer to the query using the IR system's answer_query method.\n","    answer = ir.answer_phrase_query(words)\n","    # Print out each movie that matches the query.\n","    for movie in answer:\n","        print(movie)\n"]},{"cell_type":"markdown","source":["## Testing the IR system\n","To test the system, we'll divide the original dataset into three parts: A, B, and C. Initially the index should contain A and B, then C must be added and B\n","removed (not simultaneously).\n","\n","The dataset is split according to the following percentages:\n","*   A = 50%\n","*   B = 30%\n","*   C = 20%\n","\n","To \"split\" the dataset according to the mentioned percentages we are going to use the function **read_movie_description** with the appropriate variables to create lists of **MovieDescription** objects for each section; **corpus_a**, **corpus_b** and **corpus_c**.\n","\n","We are also going to create 2 additional lists for the set of documents in **corpus_b** and **corpus_c**. In these lists, the document IDs will start from 0. This step is needed for later comparisons of the results of test queries. (This adjustment is unnecessary for documents in group A as their IDs already commence from 0.)"],"metadata":{"id":"Y0ckb7oF6M8W"}},{"cell_type":"code","source":["corpus_a = read_movie_descriptions(0.5,0)\n","corpus_b = read_movie_descriptions(0.3,0.5)\n","corpus_c = read_movie_descriptions(0.2,0.8)\n","\n","corpus_b_only = read_movie_descriptions(0.3,0.5, True)\n","corpus_c_only = read_movie_descriptions(0.2,0.8, True)"],"metadata":{"id":"w16qfjw47Lkh","executionInfo":{"status":"ok","timestamp":1705788272802,"user_tz":-60,"elapsed":9704,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":["We'll then create 4 separate IR systems.\n","\n","*   **ir**: Containing the documents from A and B initially, where we will later add C and then remove B\n","*   **irA**: Containing only the documents in A\n","*   **irB**: Containing only the documents in B\n","*   **irC**: Containing only the documents in C\n","\n","\n"],"metadata":{"id":"SpEuSPk88zfD"}},{"cell_type":"code","source":["corpus=copy.deepcopy(corpus_a)\n","for movie in corpus_b:\n","    corpus.append(movie)\n","ir=IRsystem.from_corpus(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eccS_5tE9WgA","executionInfo":{"status":"ok","timestamp":1705790636598,"user_tz":-60,"elapsed":203081,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"a53084e5-3080-415b-874e-a5b3f4448b11"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["ID: 0\n","ID: 1000\n","ID: 2000\n","ID: 3000\n","ID: 4000\n","ID: 5000\n","ID: 6000\n","ID: 7000\n","ID: 8000\n","ID: 9000\n","ID: 10000\n","ID: 11000\n","ID: 12000\n","ID: 13000\n","ID: 14000\n","ID: 15000\n","ID: 16000\n","ID: 17000\n","ID: 18000\n","ID: 19000\n","ID: 20000\n","ID: 21000\n","ID: 22000\n","ID: 23000\n","ID: 24000\n","ID: 25000\n","ID: 26000\n","ID: 27000\n","ID: 28000\n","ID: 29000\n","ID: 30000\n","ID: 31000\n","ID: 32000\n","ID: 33000\n"]}]},{"cell_type":"code","source":["irA=IRsystem.from_corpus(corpus_a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kan2wxZGGeZI","executionInfo":{"status":"ok","timestamp":1705790678377,"user_tz":-60,"elapsed":41785,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"cca19235-1601-43f9-85df-e71d0efca01b"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["ID: 0\n","ID: 1000\n","ID: 2000\n","ID: 3000\n","ID: 4000\n","ID: 5000\n","ID: 6000\n","ID: 7000\n","ID: 8000\n","ID: 9000\n","ID: 10000\n","ID: 11000\n","ID: 12000\n","ID: 13000\n","ID: 14000\n","ID: 15000\n","ID: 16000\n","ID: 17000\n","ID: 18000\n","ID: 19000\n","ID: 20000\n","ID: 21000\n"]}]},{"cell_type":"code","source":["irB=IRsystem.from_corpus(corpus_b_only)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0yKGViSGrO8","executionInfo":{"status":"ok","timestamp":1705790805661,"user_tz":-60,"elapsed":127290,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"cee607e1-1ee3-4557-aefd-0bb057479032"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["ID: 0\n","ID: 1000\n","ID: 2000\n","ID: 3000\n","ID: 4000\n","ID: 5000\n","ID: 6000\n","ID: 7000\n","ID: 8000\n","ID: 9000\n","ID: 10000\n","ID: 11000\n","ID: 12000\n"]}]},{"cell_type":"code","source":["irC=IRsystem.from_corpus(corpus_c_only)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCGlvd43GnQg","executionInfo":{"status":"ok","timestamp":1705790822361,"user_tz":-60,"elapsed":16705,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"470f53fe-5f5c-4d9e-fe09-d7e979051044"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["ID: 0\n","ID: 1000\n","ID: 2000\n","ID: 3000\n","ID: 4000\n","ID: 5000\n","ID: 6000\n","ID: 7000\n","ID: 8000\n"]}]},{"cell_type":"markdown","source":["### Initial configuration\n","Now that the IR systems have been created we can perform an initial general query in **ir**, **irA** and **irB**.\n","\n","Given that, currently, **ir** contains the documents from both **irA** and **irB**, we anticipate that the outcome of the query in **ir** will be the union of the results obtained from executing the same query in **irA** and **irB**."],"metadata":{"id":"na96GaF9GDPp"}},{"cell_type":"code","source":["binary_query(ir, \"crocodiles AND chase\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlJIsn68H4FV","executionInfo":{"status":"ok","timestamp":1705790822362,"user_tz":-60,"elapsed":16,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"908a4f84-cb9f-4ed5-f6ca-63b25d341e40"},"execution_count":117,"outputs":[{"output_type":"stream","name":"stdout","text":["Kronk's New Groove\n","Indiana Jones and the Temple of Doom\n","Kangaroo Jack\n","The Rescuers Down Under\n","Black Water\n","The Last Dragon\n","Shorts\n","The Chipmunk Adventure\n"]}]},{"cell_type":"code","source":["binary_query(irA, \"crocodiles AND chase\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUgC0V2o44Cr","executionInfo":{"status":"ok","timestamp":1705790822363,"user_tz":-60,"elapsed":13,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"35942c4c-427f-4a67-a0d1-d5ed703e18fd"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["Kronk's New Groove\n","Indiana Jones and the Temple of Doom\n","Kangaroo Jack\n","The Rescuers Down Under\n","Black Water\n","The Last Dragon\n","Shorts\n"]}]},{"cell_type":"code","source":["binary_query(irB, \"crocodiles AND chase\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uz4YYHg74xAA","executionInfo":{"status":"ok","timestamp":1705790822363,"user_tz":-60,"elapsed":9,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"2153fb5e-f792-4126-af93-822262952b5e"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["The Chipmunk Adventure\n"]}]},{"cell_type":"markdown","source":["### Adding documents\n","Let us now add the documents in C to the general IR system **ir**."],"metadata":{"id":"EKMJrKBLJh1n"}},{"cell_type":"code","source":["ir.add_documents(corpus_c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xq7h2EDGKCkR","executionInfo":{"status":"ok","timestamp":1705790833257,"user_tz":-60,"elapsed":10500,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"bd5e13b1-fb6e-4a19-856a-4758eb92e0f2"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["ID: 34000\n","ID: 35000\n","ID: 36000\n","ID: 37000\n","ID: 38000\n","ID: 39000\n","ID: 40000\n","ID: 41000\n","ID: 42000\n"]}]},{"cell_type":"markdown","source":["Now that the documents have been added to the system we can inspect the main and auxiliary indexes"],"metadata":{"id":"XCAaqMtyHz2h"}},{"cell_type":"code","source":["print(ir._index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2-_y1y0H-TZ","executionInfo":{"status":"ok","timestamp":1705790833516,"user_tz":-60,"elapsed":263,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"d9ed06fe-92f5-4b3c-da54-3bced149ca23"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["A dictionary with 170829 terms\n"]}]},{"cell_type":"code","source":["print(ir._auxiliary_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQ6VYIcsIA5M","executionInfo":{"status":"ok","timestamp":1705790833517,"user_tz":-60,"elapsed":8,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"5dfc5bc9-4ede-4ad1-c7e7-b05cf0870177"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["A dictionary with 77236 terms\n"]}]},{"cell_type":"markdown","source":["It is also possible to merge the indexes. Let's try it and then inspect the indexes again"],"metadata":{"id":"M0wZx2VFIEUl"}},{"cell_type":"code","source":["ir.merge_indexes()"],"metadata":{"id":"S_CbXkCGIPUH","executionInfo":{"status":"ok","timestamp":1705790837170,"user_tz":-60,"elapsed":3657,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"execution_count":123,"outputs":[]},{"cell_type":"code","source":["print(ir._index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDsZLU7EIRMB","executionInfo":{"status":"ok","timestamp":1705790837170,"user_tz":-60,"elapsed":16,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"aec997f6-b8f9-4191-ba2a-bb2118228145"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["A dictionary with 194757 terms\n"]}]},{"cell_type":"code","source":["print(ir._auxiliary_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPxoJRvmITCF","executionInfo":{"status":"ok","timestamp":1705790837170,"user_tz":-60,"elapsed":14,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"e73d5799-7dc7-451c-a9bd-1f833e451bc4"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["A dictionary with 0 terms\n"]}]},{"cell_type":"markdown","source":["We can now perform the same general query as before in **ir** and in **irC** and inspect the results.\n","\n","Given that, currently, **ir** contains the documents from **irA**, **irB** and **irC**, we anticipate that the outcome of the query in **ir** will be the union of the previous results and the results obtained from executing the same query in **irC**."],"metadata":{"id":"hGDLXtzPIXU3"}},{"cell_type":"code","source":["binary_query(irC, \"crocodiles AND chase\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2r01k-xgIBG2","executionInfo":{"status":"ok","timestamp":1705790837172,"user_tz":-60,"elapsed":12,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"e8c2d62a-aea5-4dde-b7dc-26b3a3cdaf33"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["Barbie as the Island Princess\n"]}]},{"cell_type":"code","source":["binary_query(ir, \"crocodiles AND chase\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SfsXY7wKGmT","executionInfo":{"status":"ok","timestamp":1705790837172,"user_tz":-60,"elapsed":9,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"14b06245-00e9-4357-997e-cb80ac0e5cfe"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stdout","text":["Kronk's New Groove\n","Indiana Jones and the Temple of Doom\n","Kangaroo Jack\n","The Rescuers Down Under\n","Black Water\n","The Last Dragon\n","Shorts\n","The Chipmunk Adventure\n","Barbie as the Island Princess\n"]}]},{"cell_type":"markdown","source":["### Deleting documents\n","Let us now delete the documents in B in the general IR system **ir**."],"metadata":{"id":"UPXcaooFK9mO"}},{"cell_type":"code","source":["ir.delete_documents(corpus_b)"],"metadata":{"id":"13ieokA-MJ4D","executionInfo":{"status":"ok","timestamp":1705790837172,"user_tz":-60,"elapsed":6,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":["To check that the documents have been correctly deleted we can perform the usual general query in **ir**.\n","\n","Given that, currently, **ir** contains the documents from both **irA** and **irC**, we anticipate that the outcome of the query in **ir** will be the difference of the previous results and the results obtained from executing the same query in **irB**."],"metadata":{"id":"BP5sttDjJLDM"}},{"cell_type":"code","source":["binary_query(ir, \"crocodiles AND chase\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcwWLhCBMOMn","executionInfo":{"status":"ok","timestamp":1705790837535,"user_tz":-60,"elapsed":369,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"6efb5be0-9fa7-41cf-84e7-ad164cc78168"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["Kronk's New Groove\n","Indiana Jones and the Temple of Doom\n","Kangaroo Jack\n","The Rescuers Down Under\n","Black Water\n","The Last Dragon\n","Shorts\n","Barbie as the Island Princess\n"]}]},{"cell_type":"markdown","source":["### Saving and Loading the index\n","The index can be saved in a .txt file whenever the user requires it with the following command"],"metadata":{"id":"3xVhihkGJpXr"}},{"cell_type":"code","source":["filename='/content/gdrive/My Drive/Information retrieval/index.txt'\n","ir.save_to_txt(filename)"],"metadata":{"id":"81iGsHrHJ2c8","executionInfo":{"status":"ok","timestamp":1705788748708,"user_tz":-60,"elapsed":22510,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":["The index can also be loaded from a file. The users has to specify the filename of the file containing the index and the corpus of documents the index refers."],"metadata":{"id":"DNk7j4WHKD67"}},{"cell_type":"code","source":["loaded_ir= IRsystem.from_index_file(filename, corpus)"],"metadata":{"id":"spW9F9fcKVgl","executionInfo":{"status":"ok","timestamp":1705788899246,"user_tz":-60,"elapsed":150540,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["### Phrase queries\n","Phrase queries can also be performed with the provided IR system."],"metadata":{"id":"gTKoQmbDKk94"}},{"cell_type":"code","source":["phrase_query(ir, \"crocodiles chase\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yo9_0ZAjM-XU","executionInfo":{"status":"ok","timestamp":1705788899246,"user_tz":-60,"elapsed":5,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"f86e19e0-8c70-494e-bf88-f5e401b288ca"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["The Rescuers Down Under\n"]}]},{"cell_type":"code","source":["binary_query(ir, \"crocodiles AND chase\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wpdPK39T5ou-","executionInfo":{"status":"ok","timestamp":1705788901265,"user_tz":-60,"elapsed":2022,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}},"outputId":"b47ef92a-0e2c-4492-a786-ff0e61299d77"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["Kronk's New Groove\n","Indiana Jones and the Temple of Doom\n","Kangaroo Jack\n","The Rescuers Down Under\n","Black Water\n","The Last Dragon\n","Shorts\n","Barbie as the Island Princess\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3UvvYrJO51oP","executionInfo":{"status":"ok","timestamp":1705788901265,"user_tz":-60,"elapsed":3,"user":{"displayName":"Noemi Ippolito","userId":"04855598929438340913"}}},"execution_count":100,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1AuUvxfKb2kAUW58EAchO95qdWXwAxSrb","timestamp":1705436923612}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"59f3145cc67fcda0343c2852f1f97113a2e6e98841e887156424448e7071ad54"}}},"nbformat":4,"nbformat_minor":0}